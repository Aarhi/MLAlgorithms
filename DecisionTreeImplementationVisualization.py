# -*- coding: utf-8 -*-
"""TSF Task 6 v1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16jLHpvzMwECw8fgP_TT9nczYShH99_I-

#Import Libraries
"""

#import required libraries
import numpy as np
import pandas as pd
from collections import Counter
#libraries for visualization of decision tree
import graphviz

"""#Read the data"""

#read the data
data_path = '/content/Iris.csv'
data = pd.read_csv(data_path, index_col='Id')

"""#Explore dataset"""

data.shape

"""Dataset has 150 rows and 5 columns"""

data.head()

"""Numerical Columns: SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm

Categorical Column: Species
"""

labels = data['Species'].unique()
labels

"""- Predict 'Species' based on values of the other columns
- Classification Problem
- 3 values of 'Species'
"""

for label in labels:
  print((data['Species'] == label).sum())

#shuffle data
orig_data = data.copy()
data = data.sample(frac=1)

#input and output
X = data.iloc[:, :-1]
y = data.iloc[:, -1]

X.shape

X.head()

y.shape

y.head()

"""#Split data into train and test sets"""

#train and test data

f = 0.75 #75% data will be used for training
total_m = data.shape[0]
train_m = int(f * total_m)
X_train = X.iloc[:train_m, :]
y_train = y.iloc[:train_m]
X_test = X.iloc[train_m:, :]
y_test = y.iloc[train_m:]

print(y_train.unique())
print(y_test.unique())

#decimal places up to which the gini index will be rounded to
r = 6

"""#Gini Index"""

#finds the gini index for a given pandas Series and round it to r
def gini(y):
    y_labels = y.unique()
    g, total = 0, 0
    for label in y_labels:
        count = (y == label).sum()
        g -= (count)**2
        total += count
    g = 1 + (g/(total**2))
    return g.round(r)

"""#Decision Tree Node class"""

#Class for the node of the decision tree
class Node:
    def __init__(self, name):
        self.name = name #name attribute is required for visualization
        self.data = 0
        self.left = None
        self.right = None
        self.isLeaf = False #true for the prediction label

"""#Decision Tree Classifier"""

#Decision tree classifier
class decisionTreeClassifier:
    
    def __init__(self):
        '''
        initializes root of the decision tree
        '''

        self.root = Node('C')
        self.dot = None #for visualization
        
    def findNode(self, X, y):       
        '''
        finds node that gives minimum gini index

        Input:
        X = input columns (pd.DataFrame)
        y = output column (pd.Series)

        Returns:
        A tuple (k, v, m) where:
        k = column name
        v = value of the column where the dataset should be split for minimum Gini index
        m = value of the Gini index for that split
        '''

        X_columns = X.columns
        final_g_arr = {}
        for col in X_columns:
            ds = X[col].copy()
            ds = ds.sort_values()
            adj_vals = set([]) #set of means of adjacent datapoints for a given column
            for i in range(1, ds.shape[0]):
                adj_vals.add(np.mean([ds.iloc[i], ds.iloc[i-1]]).round(r))
            adj_vals.discard(ds.iloc[0])
            g_arr = {}
            for val in adj_vals:
                total = ds.shape[0]
                p_yes = ((ds < val).sum() / total).round(r)
                p_no = (1 - p_yes).round(r)
                g_yes = gini(y[ds < val])
                g_no = gini(y[ds >= val])
                g = (p_yes * g_yes) + (p_no * g_no)
                g_arr[val] = g.round(r)
            if g_arr == {}:
                continue
            min_g_key = min(g_arr, key=g_arr.get) #key of the maximum value
            min_g_val = g_arr[min_g_key]
            final_g_arr[col] = (min_g_key, min_g_val)
        m = -1
        for i in final_g_arr:
            if m == -1:
                v, m = final_g_arr[i]
                k = i
            elif m > final_g_arr[i][1]:
                v, m = final_g_arr[i]
                k = i
        return (k, v, m) 
    
    def recur(self, X, y, node):
        '''
        Helper function to fit the data and create the decision tree recursively

        Input:
        X = input columns (pd.DataFrame)
        y = output column (pd.Series)
        node = Tree node which is split if required

        Returns the node which may or may not have been split
        '''
        g_before = gini(y)
        if g_before == 0:
            node.data = y.iloc[0]
            node.isLeaf = True
            return node
        col, val, G_after = self.findNode(X, y)
        if g_before <= G_after:
            node.data = Counter(y).most_common(1)[0][0]
            node.isLeaf = True
            return node
        leftX = X[X[col] < val]
        rightX = X[X[col] >= val]
        node.data = (col, val)
        node.left = Node(node.name + 'L')
        node.right = Node(node.name + 'R')
        node.left = self.recur(leftX, y[leftX.index], node.left)
        node.right = self.recur(rightX, y[rightX.index], node.right)
        return node
  
    def fit(self, X, y):
        '''
        builds the decision tree

        Input:
        X = input columns (pd.DataFrame)
        y = output column (pd.Series)
 
        '''
        self.recur(X, y, self.root)

    def recurp(self, X, node):
        '''
        Helper function to predict the output for a given input based on the decision tree created

        Input:
        X = input data (pd.Series)
        node = Tree node against which input data is checked

        Returns the output class label (string)
        '''
        if node.isLeaf:
            return node.data
        data = X[node.data[0]]
        if data < node.data[1]:
            return self.recurp(X, node.left)
        else:
            return self.recurp(X, node.right)

    def predict(self, test_data):
        '''
        predicts output for test data based on the decision tree created

        Input:
        test_data = input columns (pd.DataFrame)

        Returns output class labels for the test data (pd.Series)
        '''
        pred = []
        for i in range(test_data.shape[0]):
            pred.append(self.recurp(test_data.iloc[i], self.root))
        return pd.Series(pred, name='Species')

    def make_digraph(self, node, parent):
        '''
        Creates the visual representation of the Decision Tree's nodes and edges between the nodes

        Input:
        node = input node that will be converted into a graphviz.Digraph.node (Node)
        parent = name of the parent of the input node (string)
        '''
        if node == None:
            return
        if node.isLeaf:
            self.dot.node(node.name, node.data)
        else:
            self.dot.node(node.name, node.data[0] + ' < ' + str(node.data[1]))
        if parent:
            self.dot.edge(parent, node.name)
        self.make_digraph(node.left, node.name)
        self.make_digraph(node.right, node.name)

    def visual(self, title, filename):
        '''
        creates a file containing the image of the decision tree created

        Input:
        title = name of the Decision Tree
        filename = name of the file which will contain the image 
        '''
        self.dot = graphviz.Digraph(comment=title)
        self.make_digraph(self.root, None)
        #print(self.dot)  
        self.dot.render(filename, view=True)

"""#Accuracy function"""

def accuracy(A, B):
    '''
    returns the accuracy of the prediction
    
    Input:
    A, B = pandas.Series objects that will be compared to determine the accuracy

    Returns the accuracy rounded to 4 decimal places
    '''
    acc = 0
    rows = A.shape[0]
    for i in range(rows):
        if A.iloc[i] == B.iloc[i]:
            acc+=1
    return np.round((acc/rows), 4)

"""#Fit the model and make predictions"""

clf = decisionTreeClassifier() #make an instance of the classifier

clf.fit(X_train, y_train) #fit the classifier to the training data

predictions = clf.predict(X_test) #make predictions with the classifier on the test data

predictions.head()

"""#Find accuracy of the predictions"""

print(accuracy(predictions, y_test))

"""#Visualize the Decision Tree"""

#visualization
clf.visual('Decision Tree for Iris Dataset', 'IrisVisual.gv')